# Especificaciones del Sistema Pixely Partners

## ğŸ“‹ Documento de Especificaciones CrÃ­ticas

**Fecha:** 20 de Noviembre, 2025  
**VersiÃ³n:** 1.0  
**Estado:** Definido y Confirmado

---

## ğŸ¯ 1. MODELO MULTITENANT

### 1.1 Arquitectura de Acceso

**EspecificaciÃ³n:**
```
1 AnÃ¡lisis = 1 Cliente (Ficha Cliente) = 1 o MÃ¡s Cuentas de Acceso
```

**DefiniciÃ³n de Roles:**

| Rol | Capacidades | Restricciones |
|-----|-------------|---------------|
| **Admin** | - Crear fichas de clientes<br>- Crear usuarios<br>- Ejecutar anÃ¡lisis<br>- Visualizar todos los datos | - Acceso completo al sistema |
| **Viewer (Cliente)** | - Visualizar resultados del frontend<br>- Consultar insights de su ficha<br>- Exportar reportes | - âŒ No puede ejecutar anÃ¡lisis<br>- âŒ No puede modificar datos<br>- âŒ No tiene acceso a otros clientes |

### 1.2 Flujo de Acceso Multitenant

```mermaid
graph TD
    A[Usuario se autentica] --> B{Verificar JWT}
    B -->|Token vÃ¡lido| C[Identificar tenant_id del usuario]
    C --> D[Frontend consulta insights]
    D --> E[API filtra por tenant_id]
    E --> F[Usuario solo ve datos de su cliente]
    
    B -->|Token invÃ¡lido| G[401 Unauthorized]
```

**ImplementaciÃ³n Actual:**

1. **Base de Datos (`api/models.py`):**
   - Modelo `Tenant` como raÃ­z jerÃ¡rquica
   - Modelo `User` con FK a `tenant_id` y campo `role` (admin/user)
   - Modelo `FichaCliente` con FK a `tenant_id`
   - Todos los insights filtrados por `ficha_cliente_id` â†’ `tenant_id`

2. **AutenticaciÃ³n (`api/security.py`):**
   - JWT incluye `sub` (email del usuario) y `tenant_id`
   - Endpoint `/users/me` retorna `tenant_id` del usuario autenticado

3. **Frontend (`frontend/app.py`):**
   - âœ… Solo visualizaciÃ³n (Streamlit read-only)
   - âŒ No hay botones de "Ejecutar AnÃ¡lisis"
   - âŒ No hay formularios de ingesta
   - âœ… Todos los datos vienen de API GET endpoints

**Principio de Seguridad:**
> "El frontend es una ventana de solo lectura. La ejecuciÃ³n y modificaciÃ³n de datos es exclusiva del Orquestador automÃ¡tico y del admin vÃ­a API."

---

## â° 2. EJECUCIÃ“N AUTOMÃTICA DEL ORCHESTRATOR

### 2.1 ProgramaciÃ³n Temporal

**EspecificaciÃ³n:**
```
Frecuencia: Cada 24 horas
Horario: Configurable (ejemplo: 6:00 AM)
MÃ©todo: Cron job en contenedor orchestrator
```

**ImplementaciÃ³n:**

```dockerfile
# Dockerfile.orchestrator (lÃ­neas relevantes)
RUN apk add --no-cache dcron

# Crontab entry (ejemplo)
0 6 * * * cd /app && python -m orchestrator >> /app/orchestrator/outputs/orchestrator_debug.log 2>&1
```

### 2.2 LÃ³gica de DetecciÃ³n de Nuevos Datos

**Flujo de DecisiÃ³n:**

```python
# PseudocÃ³digo del orchestrator/__main__.py

async def main():
    # 1. Obtener Ãºltima fecha de anÃ¡lisis
    last_analysis_date = await get_last_analysis_timestamp(ficha_cliente_id)
    
    # 2. Consultar Google Sheets del cliente
    new_posts = await fetch_google_sheets_posts(spreadsheet_id)
    
    # 3. Filtrar posts nuevos
    posts_to_analyze = [
        post for post in new_posts 
        if post['created_at'] > last_analysis_date
    ]
    
    # 4. DecisiÃ³n crÃ­tica
    if len(posts_to_analyze) == 0:
        logger.info(f"No new posts found for client {ficha_cliente_id}. Skipping analysis.")
        return  # â¸ï¸ Termina sin ejecutar Q1-Q10
    
    # 5. AnÃ¡lisis incremental
    logger.info(f"Found {len(posts_to_analyze)} new posts. Starting analysis...")
    
    # 6. Ejecutar solo sobre datos nuevos
    for module in [Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q10]:
        result = await module.analyze(posts_to_analyze)
        await save_to_database(result, mode="append")  # Agregar, no reemplazar
```

**CaracterÃ­sticas Clave:**

| CaracterÃ­stica | DescripciÃ³n |
|----------------|-------------|
| **DetecciÃ³n Inteligente** | Compara `created_at` de posts vs `last_analysis_timestamp` |
| **AnÃ¡lisis Incremental** | Solo procesa posts nuevos, no reanaliza histÃ³rico |
| **Persistencia Acumulativa** | Los insights nuevos se **agregan** a la data existente |
| **Eficiencia de Costos** | âŒ No hace llamadas a OpenAI si no hay datos nuevos |
| **ActualizaciÃ³n del Timestamp** | Registra `last_analysis_timestamp` despuÃ©s de cada ejecuciÃ³n exitosa |

### 2.3 Fuente de Datos: Google Sheets

**Estructura Esperada:**

```
Spreadsheet ID: <configurado en .env por cliente>
Hoja: "Posts"

Columnas obligatorias:
- post_url (String, Ãºnico)
- platform (instagram|tiktok|facebook)
- created_at (Timestamp ISO 8601)
- content (Text)
- likes (Integer)
- comments_count (Integer)
- shares (Integer)
- views (Integer, opcional)

Hoja: "Comments"

Columnas obligatorias:
- post_url (FK a Posts.post_url)
- comment_text (Text)
- ownerUsername (String)
- created_at (Timestamp ISO 8601)
```

**IntegraciÃ³n:**

```python
# orchestrator/ingest_utils.py (conceptual)

import gspread
from oauth2client.service_account import ServiceAccountCredentials

async def fetch_google_sheets_posts(spreadsheet_id: str) -> List[Dict]:
    scope = ['https://spreadsheets.google.com/feeds']
    creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)
    client = gspread.authorize(creds)
    
    sheet = client.open_by_key(spreadsheet_id).worksheet("Posts")
    posts = sheet.get_all_records()
    
    return posts
```

### 2.4 Estrategia de Datos HistÃ³ricos

**Pregunta:** Â¿QuÃ© pasa con los insights anteriores cuando llegan posts nuevos?

**Respuesta:**

```sql
-- Estructura de social_media_insights

CREATE TABLE social_media_insights (
    id UUID PRIMARY KEY,
    ficha_cliente_id UUID REFERENCES fichas_cliente(id),
    analysis_timestamp TIMESTAMP,  -- Fecha de esta ejecuciÃ³n del orchestrator
    q1_result JSONB,                -- Resultado de Q1 (solo posts nuevos)
    q2_result JSONB,
    ...
    q10_result JSONB
);
```

**Modos de Persistencia:**

| Estrategia | DescripciÃ³n | Ventajas | Desventajas |
|------------|-------------|----------|-------------|
| **Append (Recomendado)** | Cada ejecuciÃ³n crea un nuevo registro `SocialMediaInsight` | - HistÃ³rico completo preservado<br>- AuditorÃ­a temporal<br>- Rollback posible | - Tabla crece linealmente |
| **Update** | Actualiza el registro existente, agregando nuevos posts a los arrays JSON | - 1 solo registro por cliente<br>- Queries mÃ¡s simples | - âŒ PÃ©rdida de historial<br>- âŒ No auditable |

**DecisiÃ³n de Arquitectura:**

âœ… **Modo APPEND con ventana temporal en frontend**

```python
# API endpoint para obtener insights

@app.get("/insights")
async def get_insights(
    ficha_cliente_id: str,
    start_date: Optional[datetime] = None,
    end_date: Optional[datetime] = None
):
    query = db.query(SocialMediaInsight).filter(
        SocialMediaInsight.ficha_cliente_id == ficha_cliente_id
    )
    
    if start_date:
        query = query.filter(SocialMediaInsight.analysis_timestamp >= start_date)
    
    if end_date:
        query = query.filter(SocialMediaInsight.analysis_timestamp <= end_date)
    
    # Frontend puede mostrar "Ãšltimos 30 dÃ­as" o "Todo el histÃ³rico"
    return query.order_by(SocialMediaInsight.analysis_timestamp.desc()).all()
```

---

## ğŸ” 3. SEPARACIÃ“N DE RESPONSABILIDADES

### 3.1 Matriz de Capacidades

| Componente | Puede Ejecutar AnÃ¡lisis | Puede Modificar BD | Puede Visualizar |
|------------|------------------------|-------------------|------------------|
| **Orchestrator** | âœ… AutomÃ¡tico cada 24h | âœ… VÃ­a API (POST /insights) | âŒ No |
| **API** | âŒ No | âœ… Maneja CRUD y validaciÃ³n | âŒ No |
| **Frontend** | âŒ No | âŒ No | âœ… Solo lectura |
| **Admin (vÃ­a API)** | âœ… Manual (opcional) | âœ… Puede forzar anÃ¡lisis | âœ… VÃ­a Adminer o endpoints |
| **Viewer (Frontend)** | âŒ No | âŒ No | âœ… Solo su tenant |

### 3.2 Flujo de Datos Completo

```mermaid
sequenceDiagram
    participant Cron
    participant Orchestrator
    participant GoogleSheets
    participant API
    participant DB
    participant Frontend
    participant Viewer

    Note over Cron: 6:00 AM (cada 24h)
    Cron->>Orchestrator: Trigger ejecuciÃ³n
    Orchestrator->>DB: GET /insights/last_timestamp
    DB-->>Orchestrator: 2025-11-19 06:00:00
    
    Orchestrator->>GoogleSheets: Fetch posts (created_at > last_timestamp)
    GoogleSheets-->>Orchestrator: 3 nuevos posts
    
    alt Hay posts nuevos
        Orchestrator->>Orchestrator: Ejecutar Q1-Q10 (solo 3 posts)
        Orchestrator->>API: POST /insights (payload con resultados)
        API->>API: Validar con Pydantic
        API->>DB: INSERT INTO social_media_insights
        DB-->>API: âœ… Insight guardado
        API-->>Orchestrator: 201 Created
    else No hay posts nuevos
        Orchestrator->>Orchestrator: â¸ï¸ Skip analysis
    end
    
    Note over Viewer: Usuario accede al dashboard
    Viewer->>Frontend: Abrir navegador
    Frontend->>API: GET /insights?ficha_cliente_id=X
    API->>DB: SELECT * WHERE ficha_cliente_id=X AND tenant_id=Y
    DB-->>API: Insights del cliente
    API-->>Frontend: JSON con Q1-Q10
    Frontend->>Frontend: Renderizar grÃ¡ficos Streamlit
    Frontend-->>Viewer: ğŸ“Š Dashboard visible
```

---

## ğŸ¨ 4. IMPLICACIONES DE DISEÃ‘O PARA FRONTEND

### 4.1 Restricciones de UI

**Lo que NO debe existir en el frontend:**

âŒ BotÃ³n "Ejecutar AnÃ¡lisis"  
âŒ Formulario "Subir Datos"  
âŒ Input "Agregar PublicaciÃ³n"  
âŒ Selector "Forzar Re-anÃ¡lisis"  

**Lo que SÃ debe existir:**

âœ… Selector de rango de fechas ("Ãšltimos 7 dÃ­as", "Ãšltimos 30 dÃ­as", "Todo el histÃ³rico")  
âœ… Filtros por plataforma (Instagram, TikTok, Facebook)  
âœ… Exportar a PDF/Excel  
âœ… Indicador "Ãšltima actualizaciÃ³n: hace 3 horas" (basado en `analysis_timestamp`)  

### 4.2 Ejemplo de CÃ³digo Frontend (Streamlit)

```python
# frontend/app.py

import streamlit as st
import requests
from datetime import datetime, timedelta

# AutenticaciÃ³n (ya implementada en Phase 2)
token = st.session_state.get("access_token")

# Header con timestamp de Ãºltima actualizaciÃ³n
st.title("Dashboard de AnÃ¡lisis de Redes Sociales")

# Obtener insights
response = requests.get(
    "http://api:8000/insights",
    headers={"Authorization": f"Bearer {token}"},
    params={"ficha_cliente_id": st.session_state["ficha_cliente_id"]}
)

insights = response.json()

if insights:
    last_update = max([i["analysis_timestamp"] for i in insights])
    st.caption(f"ğŸ“… Ãšltima actualizaciÃ³n: {last_update}")
else:
    st.warning("â³ Esperando primer anÃ¡lisis automÃ¡tico (se ejecuta cada 24h a las 6:00 AM)")

# Selector de rango temporal (NO ejecuta anÃ¡lisis, solo filtra visualizaciÃ³n)
date_range = st.selectbox(
    "Rango de tiempo",
    ["Ãšltimos 7 dÃ­as", "Ãšltimos 30 dÃ­as", "Todo el histÃ³rico"]
)

# Filtrar datos localmente
if date_range == "Ãšltimos 7 dÃ­as":
    cutoff = datetime.now() - timedelta(days=7)
    filtered_insights = [i for i in insights if i["analysis_timestamp"] > cutoff]
else:
    filtered_insights = insights

# Renderizar visualizaciones (solo lectura)
st.plotly_chart(create_emotion_chart(filtered_insights))
st.dataframe(create_recommendations_table(filtered_insights))
```

---

## ğŸ”§ 5. CONFIGURACIÃ“N DE ENTORNO

### 5.1 Variables de Entorno (.env)

```bash
# Orchestrator Configuration
ORCHESTRATOR_USER=admin
ORCHESTRATOR_PASSWORD=secure_password
ORCHESTRATOR_SCHEDULE="0 6 * * *"  # Cron: 6:00 AM diario

# Google Sheets Integration (por cliente)
GOOGLE_SHEETS_CLIENT_1_SPREADSHEET_ID=1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms
GOOGLE_SHEETS_CLIENT_2_SPREADSHEET_ID=...

# OpenAI (para Q1-Q10)
OPENAI_API_KEY=sk-proj-...
OPENAI_MODEL=gpt-4o-mini

# Database
DATABASE_URL=postgresql://pixely_user:secret_password_123@db:5432/pixely_db

# JWT
SECRET_KEY=pixely_partners_super_secret_key_jwt_2025
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
```

### 5.2 Estructura de Archivos CrÃ­ticos

```
pixely partners/
â”œâ”€â”€ orchestrator/
â”‚   â”œâ”€â”€ __main__.py              # Entry point del cron job
â”‚   â”œâ”€â”€ analyze.py               # Motor de anÃ¡lisis
â”‚   â”œâ”€â”€ ingest_utils.py          # IntegraciÃ³n con Google Sheets
â”‚   â”œâ”€â”€ analysis_modules/
â”‚   â”‚   â”œâ”€â”€ q1_emociones.py
â”‚   â”‚   â”œâ”€â”€ q2_personalidad.py
â”‚   â”‚   â””â”€â”€ ... (Q3-Q10)
â”‚   â””â”€â”€ outputs/
â”‚       â”œâ”€â”€ orchestrator_debug.log
â”‚       â””â”€â”€ ingested_data.json   # Cache de Ãºltima ingesta
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py                  # Endpoints REST
â”‚   â”œâ”€â”€ models.py                # ORM (Tenant, User, FichaCliente, Insights)
â”‚   â”œâ”€â”€ schemas.py               # Pydantic (Q1Response...Q10Response)
â”‚   â””â”€â”€ security.py              # JWT + bcrypt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py                   # Streamlit (solo visualizaciÃ³n)
â”‚   â””â”€â”€ view_components/
â”‚       â””â”€â”€ qual/                # Componentes de grÃ¡ficos
â”‚
â””â”€â”€ docker-compose.yml           # 5 servicios (api, db, orchestrator, frontend, adminer)
```

---

## ğŸ“Š 6. MÃ‰TRICAS Y MONITOREO

### 6.1 Logs CrÃ­ticos del Orchestrator

```python
# orchestrator/__main__.py

import logging

logger = logging.getLogger("pixely.orchestrator")
logger.setLevel(logging.INFO)

# Ejemplo de logs esperados
logger.info(f"Starting analysis for client {ficha_cliente_id}")
logger.info(f"Fetched {len(new_posts)} new posts from Google Sheets")
logger.info(f"Skipping analysis - no new data since {last_timestamp}")
logger.info(f"Module Q1 completed in {elapsed_time}s")
logger.error(f"OpenAI API failed after 3 retries: {error}")
logger.info(f"Analysis complete. Saved to database with ID {insight_id}")
```

### 6.2 Indicadores de Salud

| MÃ©trica | DÃ³nde Verificar | Valor Esperado |
|---------|----------------|----------------|
| **Ãšltima ejecuciÃ³n del orchestrator** | `orchestrator_debug.log` | < 24 horas |
| **Posts analizados hoy** | `SELECT COUNT(*) FROM social_media_insights WHERE analysis_timestamp > NOW() - INTERVAL '1 day'` | â‰¥ 1 (si hay posts nuevos) |
| **Tasa de error OpenAI** | Logs con `"OpenAI API failed"` | < 5% |
| **Tiempo de ejecuciÃ³n promedio** | Logs `"Analysis complete"` | < 15 minutos |
| **Usuarios activos frontend** | Logs de Streamlit | Datos de acceso |

---

## âœ… 7. CHECKLIST DE IMPLEMENTACIÃ“N

### Fase 1: Orchestrator AutomÃ¡tico (Pendiente)

- [ ] Implementar `orchestrator/__main__.py` con lÃ³gica de detecciÃ³n de nuevos posts
- [ ] Integrar `gspread` para lectura de Google Sheets
- [ ] Configurar cron job en `Dockerfile.orchestrator`
- [ ] Implementar `get_last_analysis_timestamp()` en API
- [ ] Probar flujo: Sin posts nuevos â†’ Skip analysis
- [ ] Probar flujo: 3 posts nuevos â†’ AnÃ¡lisis incremental

### Fase 2: Frontend Solo-Lectura (Pendiente)

- [ ] Remover cualquier botÃ³n de "Ejecutar" existente
- [ ] Implementar selector de rango temporal
- [ ] Agregar indicador "Ãšltima actualizaciÃ³n"
- [ ] Agregar mensaje "Esperando primer anÃ¡lisis" si no hay datos
- [ ] Implementar filtros de plataforma

### Fase 3: Testing Multitenant

- [ ] Crear 2 tenants de prueba (TenantA, TenantB)
- [ ] Crear 2 usuarios: userA@tenantA.com, userB@tenantB.com
- [ ] Verificar que userA solo ve datos de TenantA
- [ ] Verificar que userB no puede acceder a datos de TenantA

### Fase 4: Monitoreo

- [ ] Configurar alertas si orchestrator no ejecuta en 25 horas
- [ ] Dashboard de mÃ©tricas (opcional): Grafana + Prometheus
- [ ] Backup automÃ¡tico de PostgreSQL cada 24 horas

---

## ğŸ¯ 8. DECISIONES ESTRATÃ‰GICAS CONFIRMADAS

| DecisiÃ³n | JustificaciÃ³n |
|----------|---------------|
| **AnÃ¡lisis automÃ¡tico cada 24h** | Reduce costos de OpenAI, evita reprocesamiento innecesario |
| **Solo analizar posts nuevos** | Eficiencia computacional, respeta rate limits |
| **Frontend solo-lectura** | Seguridad, separaciÃ³n de responsabilidades |
| **Multitenant con tenant_id** | Escalabilidad, aislamiento de datos por cliente |
| **Persistencia en modo APPEND** | AuditorÃ­a temporal, histÃ³rico preservado |
| **Google Sheets como fuente** | Facilidad de ingesta por clientes, no requieren acceso directo a BD |

---

## ğŸ“ 9. PRÃ“XIMOS PASOS RECOMENDADOS

1. **Implementar lÃ³gica de detecciÃ³n de posts nuevos** en `orchestrator/__main__.py`
2. **Configurar integraciÃ³n con Google Sheets API** (credenciales de servicio)
3. **Agregar campo `last_analysis_timestamp`** en modelo `FichaCliente`
4. **Crear endpoint** `GET /fichas_cliente/{id}/last_analysis_timestamp` en API
5. **Configurar cron job** en contenedor orchestrator
6. **Probar flujo completo** con datos de prueba en Google Sheets

---

**Documento aprobado y confirmado.**
