## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- Docker & Docker Compose (for containerized deployment)
- OpenAI API Key

---

## ğŸ“¦ Setup Local Environment

### 1. Clone & Install
```bash
cd "pixely partners"
python -m venv venv
.\venv\Scripts\activate        # Windows PowerShell
# source venv/bin/activate   # Linux/Mac

pip install -r requirements.txt
```

### 2. Configure Environment
```bash
# Copy template and add your API key
copy .env.example .env
# Edit .env and set OPENAI_API_KEY=sk-...
```

### 3. Run Locally

**Option A: Run Orchestrator Only (Generate Analysis JSON)**
```bash
python orchestrator/analyze.py
# Outputs saved to orchestrator/outputs/q1_emociones.json, q2_personalidad.json, etc.
```

**Option B: Run Frontend Only (View Results)**
```bash
streamlit run frontend/app.py
# Opens http://localhost:8501
# Shows 10 analysis modules in sidebar
```

**Option C: Run Both (Full Pipeline)**
```powershell
# Terminal 1: Run orchestrator
python orchestrator/analyze.py

# Terminal 2: Run frontend
streamlit run frontend/app.py
# Visit http://localhost:8501
```

---

## ğŸ³ Docker Deployment

### 1. Build & Start Services
```bash
docker-compose up --build
# Orchestrator runs once, then Frontend starts
# Frontend available at http://localhost:8501
```

### 2. View Logs
```bash
docker-compose logs -f orchestrator
docker-compose logs -f frontend
```

### 3. Cleanup
```bash
docker-compose down
docker-compose down -v          # Also remove volumes
```

---

## ğŸ§ª Run Tests

```bash
pytest tests/ -v
# Validates all 10 modules import correctly
# Tests BaseAnalyzer, analyze.py, views
```

---

## ğŸ“Š Project Structure

```
pixely_partners/
â”œâ”€â”€ orchestrator/
â”‚   â”œâ”€â”€ base_analyzer.py           # Abstract base class
â”‚   â”œâ”€â”€ analyze.py                 # Main orchestrator
â”‚   â”œâ”€â”€ analysis_modules/          # Q1-Q10 analysis
â”‚   â”‚   â”œâ”€â”€ q1_emociones.py
â”‚   â”‚   â”œâ”€â”€ q2_personalidad.py
â”‚   â”‚   â””â”€â”€ ... (q3-q10)
â”‚   â””â”€â”€ outputs/                   # Generated JSON results
â”‚       â””â”€â”€ ingested_data.json      # Example input data
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py                     # Streamlit main
â”‚   â””â”€â”€ view_components/           # Display functions
â”‚       â”œâ”€â”€ _outputs.py
â”‚       â””â”€â”€ qual/
â”‚           â”œâ”€â”€ q1_view.py
â”‚           â””â”€â”€ ... (q2-q10)
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_imports.py            # Basic validation
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile.orchestrator
â”œâ”€â”€ Dockerfile.frontend
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md                      # Full documentation
```

---

## ğŸ“ Data Flow

1. **Input:** `orchestrator/outputs/ingested_data.json` (posts + comments)
2. **Processing:** Each Qx module analyzes independently
3. **Output:** `orchestrator/outputs/qX_name.json` (results)
4. **Display:** Frontend loads JSONs and renders via Streamlit

---

## âš ï¸ Troubleshooting

**Import errors when running locally?**
```bash
pip install -r requirements.txt --upgrade
```

**Streamlit not finding outputs?**
- Ensure orchestrator has run and created JSON files
- Check `PIXELY_OUTPUTS_DIR` in `.env`

**Docker container exits immediately?**
```bash
docker-compose logs orchestrator   # Check error message
```

---

## ğŸ”§ Next Steps

1. âœ… Add real LLM prompts to each Qx module (replace stubs)
2. âœ… Connect to real social media data source
3. âœ… Add authentication to frontend
4. âœ… Deploy to cloud (AWS/GCP/Azure)

---

## ğŸ“š Documentation

See `README.md` for comprehensive architecture, development guide, and API reference.
